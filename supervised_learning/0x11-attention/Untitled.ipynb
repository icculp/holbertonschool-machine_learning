{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731acc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kidne\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\__init__.py:140: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.16.0)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "class RNNDecoder(tf.keras.layers.Layer):\n",
    "    \"\"\" decode for machine translation \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, embedding, units, batch):\n",
    "        \"\"\"\n",
    "            vocab is an integer representing the size of the output vocabulary\n",
    "            embedding is an integer representing the dimensionality of the\n",
    "                embedding vector\n",
    "            units is an integer representing the number of hidden units in the\n",
    "                RNN cell\n",
    "            batch is an integer representing the batch size\n",
    "\n",
    "            Sets the following public instance attributes:\n",
    "            embedding - a keras Embedding layer that converts words from\n",
    "                the vocabulary into an embedding vector\n",
    "            gru - a keras GRU layer with units units\n",
    "                Should return both the full sequence of outputs as well as\n",
    "                    the last hidden state\n",
    "                Recurrent weights should be initialized with glorot_uniform\n",
    "            F - a Dense layer with vocab units\n",
    "        \"\"\"\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab,\n",
    "                                                   output_dim=embedding)\n",
    "        self.gru = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.F = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, x, s_prev, hidden_states):\n",
    "        \"\"\"\n",
    "            x is a tensor of shape (batch, 1) containing the previous word in\n",
    "                the target sequence as an index of the target vocabulary\n",
    "            s_prev is a tensor of shape (batch, units)\n",
    "                containing the previous decoder hidden state\n",
    "            hidden_states is a tensor of shape (batch, input_seq_len, units)\n",
    "                containing the outputs of the encoder\n",
    "            Returns: y, s\n",
    "            y is a tensor of shape (batch, vocab) containing the output word\n",
    "                as a one hot vector in the target vocabulary\n",
    "            s is a tensor of shape (batch, units) containing the new\n",
    "                decoder hidden state\n",
    "        \"\"\"\n",
    "        SelfAttention = __import__('1-self_attention').SelfAttention\n",
    "        context_vector, attention_weights = SelfAttention(s_prev.get_shape()[1])(s_prev, hidden_states)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        y, s = self.gru(x)\n",
    "        return y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "decoder = RNNDecoder(2048, 128, 256, 32)\n",
    "print(decoder.embedding)\n",
    "print(decoder.gru)\n",
    "print(decoder.F)\n",
    "x = tf.convert_to_tensor(np.random.choice(2048, 32).reshape((32, 1)))\n",
    "s_prev = tf.convert_to_tensor(np.random.uniform(size=(32, 256)).astype('float32'))\n",
    "hidden_states = tf.convert_to_tensor(np.random.uniform(size=(32, 10, 256)).astype('float32'))\n",
    "y, s = decoder(x, s_prev, hidden_states)\n",
    "print(y)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c766b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ceea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = SelfAttention(256)\n",
    "print(attention.W)\n",
    "print(attention.U)\n",
    "print(attention.V)\n",
    "s_prev = tf.convert_to_tensor(np.random.uniform(size=(32, 256)), preferred_dtype='float32')\n",
    "hidden_states = tf.convert_to_tensor(np.random.uniform(size=(32, 10, 256)), preferred_dtype='float32')\n",
    "context, weights = attention(s_prev, hidden_states)\n",
    "print(context)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f0462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_1.15",
   "language": "python",
   "name": "tf_gpu_1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
